{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fe25dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccb0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_excel('tweets_beforecoup.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7026516",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1[['content','date','polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af08fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d33422",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.copy()\n",
    "df1[\"polarity\"].replace({\"nt\": \"Neutral\", \"p\": \"Positive\",\"n\":\"Negative\",'np':'Negative'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3166028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df1['polarity'].value_counts(),labels=['Neutral','Positive','Negative'],autopct='%1.1f%%')\n",
    "plt.show()\n",
    "df1['polarity'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_excel('tweets_aftercoup.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7a956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2[['content','date','polarity ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f0641",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.rename(columns = {'polarity ':'polarity'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c91ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.copy()\n",
    "df2[\"polarity\"].replace({\"nt\": \"Neutral\", \"p\": \"Positive\",\"n\":\"Negative\",'np':'Negative'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e3659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df2['polarity'].value_counts(),labels=['Neutral','Positive','Negative'],autopct='%1.1f%%')\n",
    "plt.show()\n",
    "df2['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d0a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd29142",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e65a52",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f180b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c88a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = [token.lower() for token in df['content']]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a3734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "found = df[df['content'].str.contains('\\n')]\n",
    "found.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d8edf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].replace('\\n', '', regex=True)\n",
    "found = df[df['content'].str.contains('\\n')]\n",
    "found.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30419906",
   "metadata": {},
   "outputs": [],
   "source": [
    "found = df[df['content'].str.contains('@')]\n",
    "found.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9452fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].replace('@[A-Za-z0-9_]+', '', regex=True).replace('@[A-Za-z0-9_]', '', regex=True)\n",
    "found = df[df['content'].str.contains('@')]\n",
    "found.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b2b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['content'].str.contains('@')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd7bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].replace('@', '', regex=True)\n",
    "found = df[df['content'].str.contains('@')]\n",
    "found.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb929a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "found = df[df['content'].str.contains('http')]\n",
    "found.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57938839",
   "metadata": {},
   "source": [
    "# Remove Punctuations, Emojis and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5572f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['content'].copy()\n",
    "new_sent = []\n",
    "i = 0\n",
    "for sentence in sentences:\n",
    "    new_sentence = re.sub('[0-9]+', '', sentence)\n",
    "    new_sent.append(new_sentence)\n",
    "    i += 1\n",
    "    \n",
    "df['content'] = new_sent\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d5dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "sentences = df['content'].copy()\n",
    "new_sent = []\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    stripped = [w.translate(table) for w in words]\n",
    "    new_sent.append(stripped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a095f778",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = new_sent\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555fd18e",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434fa93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "from zemberek import (\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "normalizer = TurkishSentenceNormalizer(morphology)\n",
    "extractor = TurkishSentenceExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c1f3af",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120e03ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_long_text(text):\n",
    "    normalized_sentences = [normalizer.normalize(word) for word in text]\n",
    "    normalized_text = \" \".join(normalized_sentences)\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d723cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['content'].copy()\n",
    "new_sent = []\n",
    "start = time.time()\n",
    "\n",
    "for token in sentences:   \n",
    "    if token.count('') > 0:\n",
    "        token = list(filter(('').__ne__, token))\n",
    "    new_token = normalize_long_text(token)\n",
    "    new_sent.append(new_token)\n",
    "\n",
    "logger.info(f\"Sentences normalized in: {time.time() - start} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8639fb09",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3612e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stops = set(stopwords.words('turkish'))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c4500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_words = []\n",
    "for sent in new_sent:\n",
    "    words = sent.split()\n",
    "    splitted_words.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c4af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sent = []\n",
    "for sentence in splitted_words:\n",
    "    new_sentence = [w for w in sentence if w not in stops]\n",
    "    clean_sent.append(new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad845034",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70869087",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in clean_sent:\n",
    "    j = 0\n",
    "    for word in token:\n",
    "        new_word = word.replace('\"', '').replace(\"’\", '').replace(\"'\", '').replace(\"”\", '')\n",
    "        token[j] = new_word\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zeyrek\n",
    "\n",
    "\n",
    "analyzer = zeyrek.MorphAnalyzer()\n",
    "lem_sent = []\n",
    "for sent in clean_sent:\n",
    "    normalized_sent = []\n",
    "    for word in sent:\n",
    "        if word == '':\n",
    "            continue\n",
    "        else:\n",
    "            lem_word = analyzer.lemmatize(word)\n",
    "            normalized_sent.append(lem_word[0][1][0])\n",
    "    lem_sent.append(normalized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfadab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lem_sent.copy()\n",
    "for sent in x:\n",
    "    i = 0\n",
    "    for token in sent:\n",
    "        sent[i] = token.lower()\n",
    "        i += 1\n",
    "lem_sent = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1562c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_sent = list(filter(('').__ne__, lem_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = lem_sent\n",
    "df['content'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01b3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df.content.apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f60a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweets_lemmatized.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
